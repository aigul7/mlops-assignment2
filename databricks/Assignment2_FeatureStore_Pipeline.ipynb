{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183182f0-5573-4413-a91c-6d04c82db306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating athletes dataset in Databricks...\nCreated table 'default.athletes' with 1000 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>athlete_id</th><th>name</th><th>age</th><th>height</th><th>weight</th><th>country</th><th>sport</th><th>years_experience</th><th>training_hours_per_week</th><th>medal</th></tr></thead><tbody><tr><td>1</td><td>Athlete_1</td><td>24</td><td>166.251932862553</td><td>69.87891459741175</td><td>China</td><td>Rowing</td><td>17</td><td>25</td><td>Bronze</td></tr><tr><td>2</td><td>Athlete_2</td><td>37</td><td>170.99627126832448</td><td>68.4979801479763</td><td>Japan</td><td>Swimming</td><td>13</td><td>31</td><td>None</td></tr><tr><td>3</td><td>Athlete_3</td><td>32</td><td>180.09378116336106</td><td>82.59554101811791</td><td>Russia</td><td>Swimming</td><td>13</td><td>12</td><td>None</td></tr><tr><td>4</td><td>Athlete_4</td><td>28</td><td>187.47327055231725</td><td>69.60508754129663</td><td>Australia</td><td>Swimming</td><td>7</td><td>33</td><td>None</td></tr><tr><td>5</td><td>Athlete_5</td><td>25</td><td>184.6592200795959</td><td>87.21001474901018</td><td>Russia</td><td>Athletics</td><td>10</td><td>21</td><td>Bronze</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Athlete_1",
         24,
         166.251932862553,
         69.87891459741175,
         "China",
         "Rowing",
         17,
         25,
         "Bronze"
        ],
        [
         2,
         "Athlete_2",
         37,
         170.99627126832448,
         68.4979801479763,
         "Japan",
         "Swimming",
         13,
         31,
         "None"
        ],
        [
         3,
         "Athlete_3",
         32,
         180.09378116336106,
         82.59554101811791,
         "Russia",
         "Swimming",
         13,
         12,
         "None"
        ],
        [
         4,
         "Athlete_4",
         28,
         187.47327055231725,
         69.60508754129663,
         "Australia",
         "Swimming",
         7,
         33,
         "None"
        ],
        [
         5,
         "Athlete_5",
         25,
         184.6592200795959,
         87.21001474901018,
         "Russia",
         "Athletics",
         10,
         21,
         "Bronze"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "athlete_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "height",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "weight",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sport",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "years_experience",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "training_hours_per_week",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "medal",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the athletes dataset directly in Databricks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "print(\"Creating athletes dataset in Databricks...\")\n",
    "\n",
    "# Create data\n",
    "data = {\n",
    "    'athlete_id': range(1, n + 1),\n",
    "    'name': [f'Athlete_{i}' for i in range(1, n + 1)],\n",
    "    'age': np.random.randint(18, 40, n),\n",
    "    'height': np.random.normal(175, 10, n),\n",
    "    'weight': np.random.normal(75, 12, n),\n",
    "    'country': np.random.choice(['USA', 'China', 'Russia', 'Germany', 'UK', 'France', 'Japan', 'Australia'], n),\n",
    "    'sport': np.random.choice(['Swimming', 'Athletics', 'Gymnastics', 'Cycling', 'Rowing'], n),\n",
    "    'years_experience': np.random.randint(1, 20, n),\n",
    "    'training_hours_per_week': np.random.randint(10, 40, n),\n",
    "}\n",
    "\n",
    "df_pandas = pd.DataFrame(data)\n",
    "\n",
    "# Target variable\n",
    "medals = ['Gold'] * 100 + ['Silver'] * 150 + ['Bronze'] * 200 + ['None'] * 550\n",
    "np.random.shuffle(medals)\n",
    "df_pandas['medal'] = medals\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Save as table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"default.athletes\")\n",
    "\n",
    "print(f\"Created table 'default.athletes' with {df.count()} rows\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbdac94-de24-4fe4-b8ed-ea60f71d6d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: codecarbon in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (3.2.2)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (1.6.1)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.12/site-packages (3.10.0)\nRequirement already satisfied: seaborn in /databricks/python3/lib/python3.12/site-packages (0.13.2)\nRequirement already satisfied: arrow in /databricks/python3/lib/python3.12/site-packages (from codecarbon) (1.3.0)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from codecarbon) (8.1.7)\nRequirement already satisfied: fief-client[cli] in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from codecarbon) (0.20.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from codecarbon) (2.2.3)\nRequirement already satisfied: prometheus_client in /databricks/python3/lib/python3.12/site-packages (from codecarbon) (0.21.0)\nRequirement already satisfied: psutil>=6.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from codecarbon) (7.2.2)\nRequirement already satisfied: py-cpuinfo in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from codecarbon) (9.0.0)\nRequirement already satisfied: pydantic in /databricks/python3/lib/python3.12/site-packages (from codecarbon) (2.10.6)\nRequirement already satisfied: nvidia-ml-py in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from codecarbon) (13.590.48)\nRequirement already satisfied: rapidfuzz in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from codecarbon) (3.14.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from codecarbon) (2.32.3)\nRequirement already satisfied: questionary in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from codecarbon) (2.1.1)\nRequirement already satisfied: rich in /databricks/python3/lib/python3.12/site-packages (from codecarbon) (13.9.4)\nRequirement already satisfied: typer in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from codecarbon) (0.21.1)\nRequirement already satisfied: numpy>=1.19.5 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (24.1)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->codecarbon) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas->codecarbon) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /databricks/python3/lib/python3.12/site-packages (from arrow->codecarbon) (2.9.0.20241206)\nRequirement already satisfied: httpx<0.28.0,>=0.21.3 in /databricks/python3/lib/python3.12/site-packages (from fief-client[cli]->codecarbon) (0.27.0)\nRequirement already satisfied: jwcrypto<2.0.0,>=1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from fief-client[cli]->codecarbon) (1.5.6)\nRequirement already satisfied: yaspin in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from fief-client[cli]->codecarbon) (3.4.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic->codecarbon) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic->codecarbon) (2.27.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic->codecarbon) (4.12.2)\nRequirement already satisfied: prompt_toolkit<4.0,>=2.0 in /databricks/python3/lib/python3.12/site-packages (from questionary->codecarbon) (3.0.43)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->codecarbon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->codecarbon) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->codecarbon) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->codecarbon) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich->codecarbon) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich->codecarbon) (2.15.1)\nRequirement already satisfied: shellingham>=1.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from typer->codecarbon) (1.5.4)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.6.2)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.2)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\nRequirement already satisfied: cryptography>=3.4 in /databricks/python3/lib/python3.12/site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\nRequirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.12/site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.5)\nRequirement already satisfied: termcolor<4.0,>=3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f40d9f24-ae1d-4790-982d-eae5dceb97ec/lib/python3.12/site-packages (from yaspin->fief-client[cli]->codecarbon) (3.3.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install CodeCarbon for carbon tracking\n",
    "%pip install codecarbon scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7cdb24e-c3fc-4a4a-a06b-41a521086288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2882618-8c04-4134-be12-cf592c789a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 1000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>athlete_id</th><th>name</th><th>age</th><th>height</th><th>weight</th><th>country</th><th>sport</th><th>years_experience</th><th>training_hours_per_week</th><th>medal</th><th>won_medal</th><th>country_encoded</th><th>sport_encoded</th></tr></thead><tbody><tr><td>72</td><td>Athlete_72</td><td>31</td><td>173.35464341950305</td><td>77.89485765186417</td><td>Australia</td><td>Athletics</td><td>15</td><td>19</td><td>None</td><td>0</td><td>0</td><td>0</td></tr><tr><td>32</td><td>Athlete_32</td><td>29</td><td>201.4357158339855</td><td>76.13992501627565</td><td>Australia</td><td>Athletics</td><td>3</td><td>31</td><td>None</td><td>0</td><td>0</td><td>0</td></tr><tr><td>28</td><td>Athlete_28</td><td>33</td><td>183.35565398349905</td><td>68.8747673708764</td><td>Australia</td><td>Athletics</td><td>13</td><td>37</td><td>None</td><td>0</td><td>0</td><td>0</td></tr><tr><td>88</td><td>Athlete_88</td><td>18</td><td>177.0370179813885</td><td>78.65031503464105</td><td>Australia</td><td>Athletics</td><td>7</td><td>25</td><td>None</td><td>0</td><td>0</td><td>0</td></tr><tr><td>134</td><td>Athlete_134</td><td>24</td><td>181.72257284413772</td><td>76.80163339645661</td><td>Australia</td><td>Athletics</td><td>4</td><td>27</td><td>None</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         72,
         "Athlete_72",
         31,
         173.35464341950305,
         77.89485765186417,
         "Australia",
         "Athletics",
         15,
         19,
         "None",
         0,
         0,
         0
        ],
        [
         32,
         "Athlete_32",
         29,
         201.4357158339855,
         76.13992501627565,
         "Australia",
         "Athletics",
         3,
         31,
         "None",
         0,
         0,
         0
        ],
        [
         28,
         "Athlete_28",
         33,
         183.35565398349905,
         68.8747673708764,
         "Australia",
         "Athletics",
         13,
         37,
         "None",
         0,
         0,
         0
        ],
        [
         88,
         "Athlete_88",
         18,
         177.0370179813885,
         78.65031503464105,
         "Australia",
         "Athletics",
         7,
         25,
         "None",
         0,
         0,
         0
        ],
        [
         134,
         "Athlete_134",
         24,
         181.72257284413772,
         76.80163339645661,
         "Australia",
         "Athletics",
         4,
         27,
         "None",
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "athlete_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "height",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "weight",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sport",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "years_experience",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "training_hours_per_week",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "medal",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "won_medal",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "country_encoded",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "sport_encoded",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load athletes table\n",
    "df = spark.table(\"default.athletes\")\n",
    "\n",
    "# Create target variable\n",
    "df = df.withColumn(\"won_medal\", F.when(F.col(\"medal\") != \"None\", 1).otherwise(0))\n",
    "\n",
    "# Encode country using simpler approach (serverless compatible)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "# Country encoding\n",
    "country_window = Window.orderBy(\"country\")\n",
    "df = df.withColumn(\"country_rank\", dense_rank().over(country_window))\n",
    "df = df.withColumn(\"country_encoded\", F.col(\"country_rank\") - 1)\n",
    "df = df.drop(\"country_rank\")\n",
    "\n",
    "# Sport encoding\n",
    "sport_window = Window.orderBy(\"sport\")\n",
    "df = df.withColumn(\"sport_rank\", dense_rank().over(sport_window))\n",
    "df = df.withColumn(\"sport_encoded\", F.col(\"sport_rank\") - 1)\n",
    "df = df.drop(\"sport_rank\")\n",
    "\n",
    "print(f\"Loaded rows: {df.count()}\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ab2893-a0f7-460b-99f5-21bd4b2b4693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Table V1 created\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>athlete_id</th><th>age</th><th>height</th><th>weight</th><th>years_experience</th><th>training_hours_per_week</th><th>country_encoded</th><th>sport_encoded</th></tr></thead><tbody><tr><td>72</td><td>31.0</td><td>173.35464</td><td>77.89486</td><td>15.0</td><td>19.0</td><td>0</td><td>0</td></tr><tr><td>32</td><td>29.0</td><td>201.43571</td><td>76.13992</td><td>3.0</td><td>31.0</td><td>0</td><td>0</td></tr><tr><td>28</td><td>33.0</td><td>183.35565</td><td>68.87477</td><td>13.0</td><td>37.0</td><td>0</td><td>0</td></tr><tr><td>88</td><td>18.0</td><td>177.03702</td><td>78.650314</td><td>7.0</td><td>25.0</td><td>0</td><td>0</td></tr><tr><td>134</td><td>24.0</td><td>181.72258</td><td>76.801636</td><td>4.0</td><td>27.0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         72,
         31.0,
         173.35464,
         77.89486,
         15.0,
         19.0,
         0,
         0
        ],
        [
         32,
         29.0,
         201.43571,
         76.13992,
         3.0,
         31.0,
         0,
         0
        ],
        [
         28,
         33.0,
         183.35565,
         68.87477,
         13.0,
         37.0,
         0,
         0
        ],
        [
         88,
         18.0,
         177.03702,
         78.650314,
         7.0,
         25.0,
         0,
         0
        ],
        [
         134,
         24.0,
         181.72258,
         76.801636,
         4.0,
         27.0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "athlete_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "height",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "weight",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "years_experience",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "training_hours_per_week",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "country_encoded",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "sport_encoded",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Version 1: Basic features\n",
    "features_v1 = df.select(\n",
    "    \"athlete_id\",\n",
    "    F.col(\"age\").cast(\"float\"),\n",
    "    F.col(\"height\").cast(\"float\"),\n",
    "    F.col(\"weight\").cast(\"float\"),\n",
    "    F.col(\"years_experience\").cast(\"float\"),\n",
    "    F.col(\"training_hours_per_week\").cast(\"float\"),\n",
    "    F.col(\"country_encoded\").cast(\"int\"),\n",
    "    F.col(\"sport_encoded\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Save as table\n",
    "features_v1.write.mode(\"overwrite\").saveAsTable(\"default.athlete_features_v1\")\n",
    "\n",
    "print(\"Feature Table V1 created\")\n",
    "display(features_v1.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0158ca2-58f9-49c2-afd4-8a779e87a58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Table V2 created\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>athlete_id</th><th>age</th><th>bmi</th><th>experience_per_age</th><th>training_intensity</th><th>age_group</th><th>height_weight_ratio</th><th>country_encoded</th><th>sport_encoded</th></tr></thead><tbody><tr><td>72</td><td>31.0</td><td>25.920169069143732</td><td>0.4838709677419355</td><td>1.1875</td><td>2</td><td>2.2254953490547185</td><td>0</td><td>0</td></tr><tr><td>32</td><td>29.0</td><td>18.764607830510926</td><td>0.10344827586206896</td><td>7.75</td><td>1</td><td>2.645599083410269</td><td>0</td><td>0</td></tr><tr><td>28</td><td>33.0</td><td>20.486677556500418</td><td>0.3939393939393939</td><td>2.642857142857143</td><td>2</td><td>2.6621600476146328</td><td>0</td><td>0</td></tr><tr><td>88</td><td>18.0</td><td>25.094138759460694</td><td>0.3888888888888889</td><td>3.125</td><td>0</td><td>2.250938447015929</td><td>0</td><td>0</td></tr><tr><td>134</td><td>24.0</td><td>23.256946972708256</td><td>0.16666666666666666</td><td>5.4</td><td>0</td><td>2.366129010643175</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         72,
         31.0,
         25.920169069143732,
         0.4838709677419355,
         1.1875,
         2,
         2.2254953490547185,
         0,
         0
        ],
        [
         32,
         29.0,
         18.764607830510926,
         0.10344827586206896,
         7.75,
         1,
         2.645599083410269,
         0,
         0
        ],
        [
         28,
         33.0,
         20.486677556500418,
         0.3939393939393939,
         2.642857142857143,
         2,
         2.6621600476146328,
         0,
         0
        ],
        [
         88,
         18.0,
         25.094138759460694,
         0.3888888888888889,
         3.125,
         0,
         2.250938447015929,
         0,
         0
        ],
        [
         134,
         24.0,
         23.256946972708256,
         0.16666666666666666,
         5.4,
         0,
         2.366129010643175,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "athlete_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "bmi",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "experience_per_age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "training_intensity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "age_group",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "height_weight_ratio",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "country_encoded",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "sport_encoded",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Version 2: Engineered features\n",
    "features_v2 = df.select(\n",
    "    \"athlete_id\", \"age\", \"height\", \"weight\", \"years_experience\", \n",
    "    \"training_hours_per_week\", \"country_encoded\", \"sport_encoded\"\n",
    ").withColumn(\"age\", F.col(\"age\").cast(\"float\")) \\\n",
    " .withColumn(\"bmi\", F.col(\"weight\") / F.pow(F.col(\"height\") / 100, 2)) \\\n",
    " .withColumn(\"experience_per_age\", F.col(\"years_experience\") / F.col(\"age\")) \\\n",
    " .withColumn(\"training_intensity\", F.col(\"training_hours_per_week\") / (F.col(\"years_experience\") + 1)) \\\n",
    " .withColumn(\"age_group\", \n",
    "     F.when(F.col(\"age\") <= 25, 0)\n",
    "      .when((F.col(\"age\") > 25) & (F.col(\"age\") <= 30), 1)\n",
    "      .when((F.col(\"age\") > 30) & (F.col(\"age\") <= 35), 2)\n",
    "      .otherwise(3)) \\\n",
    " .withColumn(\"height_weight_ratio\", F.col(\"height\") / F.col(\"weight\")) \\\n",
    " .select(\"athlete_id\", \"age\", \"bmi\", \"experience_per_age\", \"training_intensity\", \n",
    "         \"age_group\", \"height_weight_ratio\", \"country_encoded\", \"sport_encoded\")\n",
    "\n",
    "# Save as table\n",
    "features_v2.write.mode(\"overwrite\").saveAsTable(\"default.athlete_features_v2\")\n",
    "\n",
    "print(\"Feature Table V2 created\")\n",
    "display(features_v2.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99de3f56-73c4-450d-b038-ebd3b35fcbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function train_model ready\n"
     ]
    }
   ],
   "source": [
    "def train_model(features_df, target_df, feature_version, n_estimators, max_depth, exp_name):\n",
    "    \"\"\"Train model with MLflow and CodeCarbon tracking\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"TRAINING: {exp_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Merge features and target\n",
    "    data = features_df.merge(target_df, on=\"athlete_id\")\n",
    "    X = data.drop([\"athlete_id\", \"won_medal\"], axis=1)\n",
    "    y = data[\"won_medal\"]\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # Start CodeCarbon tracking\n",
    "    tracker = EmissionsTracker(project_name=exp_name, log_level=\"error\")\n",
    "    tracker.start()\n",
    "    \n",
    "    # MLflow run\n",
    "    with mlflow.start_run(run_name=exp_name):\n",
    "        mlflow.log_param(\"feature_version\", feature_version)\n",
    "        mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators, \n",
    "            max_depth=max_depth, \n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        y_proba = y_proba[:, 1] if y_proba.shape[1] == 2 else y_proba[:, 0]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        roc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0.5\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision\", prec)\n",
    "        mlflow.log_metric(\"recall\", rec)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc)\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, ROC AUC: {roc:.4f}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {exp_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        mlflow.log_figure(plt.gcf(), f\"confusion_matrix_{exp_name}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Feature Importance\n",
    "        imp = pd.DataFrame({\n",
    "            'feature': X.columns, \n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(data=imp.head(10), x='importance', y='feature')\n",
    "        plt.title(f'Feature Importance - {exp_name}')\n",
    "        mlflow.log_figure(plt.gcf(), f\"feature_importance_{exp_name}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Stop carbon tracking\n",
    "        emissions = tracker.stop()\n",
    "        mlflow.log_metric(\"carbon_emissions_kg\", emissions)\n",
    "        print(f\"Carbon Emissions: {emissions:.6f} kg CO2\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        \"experiment_name\": exp_name, \n",
    "        \"feature_version\": feature_version, \n",
    "        \"n_estimators\": n_estimators, \n",
    "        \"max_depth\": max_depth, \n",
    "        \"accuracy\": acc, \n",
    "        \"f1_score\": f1,\n",
    "        \"roc_auc\": roc,\n",
    "        \"carbon_emissions_kg\": emissions\n",
    "    }\n",
    "\n",
    "print(\"Function train_model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc172fd4-ec6a-4361-b91a-baf7c9efd064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n2026/02/09 21:48:15 INFO mlflow.tracking.fluent: Experiment with name '/Users/aigul.azamat7@gmail.com/athlete_prediction' does not exist. Creating a new experiment.\n[codecarbon WARNING @ 21:48:15] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nRUNNING 4 EXPERIMENTS\n\n[1/4] Experiment 1: V1 + HP1...\n======================================================================\nTRAINING: exp1_v1_hp1\n======================================================================\nTrain: (800, 7), Test: (200, 7)\nAccuracy: 0.4750, F1: 0.2953, ROC AUC: 0.4976\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/02/09 21:48:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon Emissions: 0.000009 kg CO2\n======================================================================\n\n[2/4] Experiment 2: V1 + HP2...\n======================================================================\nTRAINING: exp2_v1_hp2\n======================================================================\nTrain: (800, 7), Test: (200, 7)\nAccuracy: 0.4850, F1: 0.3522, ROC AUC: 0.4785\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/02/09 21:48:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon Emissions: 0.000005 kg CO2\n======================================================================\n\n[3/4] Experiment 3: V2 + HP1...\n======================================================================\nTRAINING: exp3_v2_hp1\n======================================================================\nTrain: (800, 8), Test: (200, 8)\nAccuracy: 0.5400, F1: 0.3947, ROC AUC: 0.5201\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/02/09 21:48:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon Emissions: 0.000006 kg CO2\n======================================================================\n\n[4/4] Experiment 4: V2 + HP2...\n======================================================================\nTRAINING: exp4_v2_hp2\n======================================================================\nTrain: (800, 8), Test: (200, 8)\nAccuracy: 0.5350, F1: 0.4294, ROC AUC: 0.5299\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/02/09 21:48:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon Emissions: 0.000007 kg CO2\n======================================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>experiment_name</th><th>feature_version</th><th>n_estimators</th><th>max_depth</th><th>accuracy</th><th>f1_score</th><th>roc_auc</th><th>carbon_emissions_kg</th></tr></thead><tbody><tr><td>exp1_v1_hp1</td><td>v1</td><td>100</td><td>10</td><td>0.475</td><td>0.2953020134228188</td><td>0.4975757575757575</td><td>8.71059703869429E-6</td></tr><tr><td>exp2_v1_hp2</td><td>v1</td><td>200</td><td>20</td><td>0.485</td><td>0.3522012578616352</td><td>0.4784848484848484</td><td>5.418676692254616E-6</td></tr><tr><td>exp3_v2_hp1</td><td>v2</td><td>100</td><td>10</td><td>0.54</td><td>0.39473684210526316</td><td>0.5201010101010101</td><td>6.143828597676312E-6</td></tr><tr><td>exp4_v2_hp2</td><td>v2</td><td>200</td><td>20</td><td>0.535</td><td>0.4294478527607362</td><td>0.5298989898989899</td><td>6.975094372777969E-6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "exp1_v1_hp1",
         "v1",
         100,
         10,
         0.475,
         0.2953020134228188,
         0.4975757575757575,
         8.71059703869429E-6
        ],
        [
         "exp2_v1_hp2",
         "v1",
         200,
         20,
         0.485,
         0.3522012578616352,
         0.4784848484848484,
         5.418676692254616E-6
        ],
        [
         "exp3_v2_hp1",
         "v2",
         100,
         10,
         0.54,
         0.39473684210526316,
         0.5201010101010101,
         6.143828597676312E-6
        ],
        [
         "exp4_v2_hp2",
         "v2",
         200,
         20,
         0.535,
         0.4294478527607362,
         0.5298989898989899,
         6.975094372777969E-6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "experiment_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "feature_version",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_estimators",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "max_depth",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "accuracy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "f1_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "roc_auc",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "carbon_emissions_kg",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "target_df = df.select(\"athlete_id\", \"won_medal\").toPandas()\n",
    "features_v1_pd = spark.table(\"default.athlete_features_v1\").toPandas()\n",
    "features_v2_pd = spark.table(\"default.athlete_features_v2\").toPandas()\n",
    "\n",
    "# Set MLflow experiment - IMPORTANT: Replace YOUR_EMAIL with your actual email\n",
    "mlflow.set_experiment(\"/Users/aigul.azamat7@gmail.com/athlete_prediction\")\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nRUNNING 4 EXPERIMENTS\\n\")\n",
    "\n",
    "print(\"[1/4] Experiment 1: V1 + HP1...\")\n",
    "results.append(train_model(features_v1_pd, target_df, \"v1\", 100, 10, \"exp1_v1_hp1\"))\n",
    "\n",
    "print(\"\\n[2/4] Experiment 2: V1 + HP2...\")\n",
    "results.append(train_model(features_v1_pd, target_df, \"v1\", 200, 20, \"exp2_v1_hp2\"))\n",
    "\n",
    "print(\"\\n[3/4] Experiment 3: V2 + HP1...\")\n",
    "results.append(train_model(features_v2_pd, target_df, \"v2\", 100, 10, \"exp3_v2_hp1\"))\n",
    "\n",
    "print(\"\\n[4/4] Experiment 4: V2 + HP2...\")\n",
    "results.append(train_model(features_v2_pd, target_df, \"v2\", 200, 20, \"exp4_v2_hp2\"))\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05663df5-875e-44b6-9d5d-d0ad7b3438b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: exp4_v2_hp2\nF1 Score: 0.4294\nAccuracy: 0.5350\nROC AUC: 0.5299\nCarbon Emissions: 0.000007 kg CO2\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(\"/Users/aigul.azamat7@gmail.com/athlete_prediction\")\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[exp.experiment_id],\n",
    "    order_by=[\"metrics.f1_score DESC\", \"metrics.accuracy DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "if runs:\n",
    "    best = runs[0]\n",
    "\n",
    "    print(f\"Name: {best.data.tags.get('mlflow.runName', '')}\")\n",
    "    print(f\"F1 Score: {best.data.metrics.get('f1_score', 0):.4f}\")\n",
    "    print(f\"Accuracy: {best.data.metrics.get('accuracy', 0):.4f}\")\n",
    "    print(f\"ROC AUC: {best.data.metrics.get('roc_auc', 0):.4f}\")\n",
    "    print(f\"Carbon Emissions: {best.data.metrics.get('carbon_emissions_kg', 0):.6f} kg CO2\")    \n",
    "    model = mlflow.sklearn.load_model(f\"runs:/{best.info.run_id}/model\")\n",
    "\n",
    "else:\n",
    "    print(\"No experiments found\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment2_FeatureStore_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}